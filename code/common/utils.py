import sys, os
sys.path.append(os.path.dirname(__file__))
sys.stdout.reconfigure(encoding='utf-8')

import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import re
from common.constants import Panel
from common.constants import Iloc
import db

def nowTime():
    """å–å¾—ç•¶å‰æ™‚é–“ (yyyy/mm/dd hh:mm:ss)"""
    return datetime.now().strftime("%Y/%m/%d %H:%M:%S")

def ptMsg(msg, msg2=None):
    """æ‰“å°æ™‚é–“èˆ‡æ—¥èªŒ (yyyy/mm/dd hh:mm:ss)"""
    
    print(f"{nowTime()}ï¼š{msg}")
    if msg2 is not None:
        print(msg2)

def inTimeRange(targetDt: datetime, sDt: datetime , eDt: datetime) -> bool:
    return sDt <= targetDt <= eDt

def getSdtEdt(filePath: str) -> dict[str, datetime]:
    filename = Path(filePath).stem # å»é™¤é™„æª”åå¾Œçš„æª”å

    # æ‰¾åˆ°å…©çµ„6ä½æ•¸å­—
    matches = re.findall(r'(\d{6})', filename)

    start_str = matches[0]
    end_str = matches[1]

    sDt = datetime.strptime(start_str + '01', "%Y%m%d") # Start Date
    eDt = datetime.strptime(end_str + '01', "%Y%m%d") # End Date

    eDt = eDt + pd.offsets.MonthEnd(0) # æ™‚é–“æ¨ç§»åˆ°æœˆåº•

    result = {
        "sDt": sDt,
        "eDt": eDt
    }
    return result

def getCloseDf(sYear: str , searchY: int) -> pd.DataFrame:
    base_dir = f'../data/analysis/summary/closePrice'
    sYear_int = int(sYear)
    
    # ç”¢ç”Ÿä¸‰å¹´çš„æª”æ¡ˆæ¸…å–®
    years = [str(sYear_int + i) for i in range(searchY)]
    filenames = [f'closePrice_{y}.csv' for y in years]
    filepaths = [os.path.join(base_dir, fname) for fname in filenames]
    
    close_dfs = []
    for fp in filepaths:
        if os.path.exists(fp):
            try:
                df = pd.read_csv(fp, parse_dates=['date'], dtype={'stock_id': str})
                close_dfs.append(df)
                print(f"è®€å–æª”æ¡ˆï¼š{fp}ï¼Œç­†æ•¸ï¼š{len(df)}")
            except Exception as e:
                print(f"è®€å–æª”æ¡ˆ {fp} ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        else:
            print(f"æª”æ¡ˆä¸å­˜åœ¨ï¼š{fp}")

    # åˆä½µæˆ–å»ºç«‹ç©º DataFrame
    if close_dfs:
        close_df = pd.concat(close_dfs, ignore_index=True)
    else:
        close_df = pd.DataFrame()

    print(f"closePriceï¼šå¹´ä»½ {sYear} ~ {int(sYear) + searchY - 1} åˆä½µè³‡æ–™ç­†æ•¸ï¼š{len(close_df)}")
    # print(close_df.head(3))
    ## for test
    close_df.to_csv(f"{base_dir}/closePrice_tmp.csv")
    return close_df
    
def delete_empty_csv_files(folder_path):
    deleted_files = []

    for filename in os.listdir(folder_path):
        if filename.lower().endswith('.csv'):
            filepath = os.path.join(folder_path, filename)
            try:
                with open(filepath, encoding="utf-8") as f:
                    lines = [line.strip() for line in f if line.strip()]
                    if not lines:
                        os.remove(filepath)
                        deleted_files.append(filename)
                        print(f"å·²åˆªé™¤ç©ºæª”æ¡ˆï¼š{filename}")
            except Exception as e:
                print(f"è®€å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{filename}ï¼ŒåŸå› ï¼š{e}")

    print(f"\nç¸½å…±åˆªé™¤ {len(deleted_files)} å€‹ç©ºç™½CSVæª”æ¡ˆã€‚")
    return deleted_files
def is_really_empty_file(filepath):
    """å¼·åŒ–ç‰ˆæœ¬ï¼šæ•´ä»½æª”æ¡ˆå»é™¤ç©ºç™½ã€æ›è¡Œã€BOMã€åˆ¶è¡¨ç¬¦å¾Œï¼Œç¢ºèªæ˜¯å¦å®Œå…¨ç„¡å…§å®¹"""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()
            cleaned_content = content.replace("\n", "").replace("\r", "").replace("\t", "").strip()
            return len(cleaned_content) == 0
    except Exception as e:
        print(f"æª¢æŸ¥å¤±æ•—ï¼š{filepath}ï¼ŒåŸå› ï¼š{e}")
        return False

def delete_empty_csv_files_recursive(folder_path, size_threshold=2*1024):
    """æª”æ¡ˆå¤§å°å°ä¸”å…§å®¹ç´”ç©ºç™½ï¼Œå³åˆªé™¤"""
    deleted_files = []
    checked_files = 0

    for root, dirs, files in os.walk(folder_path):
        for filename in files:
            if filename.lower().endswith('.csv'):
                filepath = os.path.join(root, filename)
                checked_files += 1

                try:
                    # æª”æ¡ˆéå°æ‰é€²ä¸€æ­¥æª¢æŸ¥å…§å®¹
                    if os.path.getsize(filepath) <= size_threshold:
                        if is_really_empty_file(filepath):
                            os.remove(filepath)
                            deleted_files.append(filepath)
                            print(f"å·²åˆªé™¤ç´”ç©ºç™½æª”æ¡ˆï¼š{filepath}")
                except Exception as e:
                    print(f"è™•ç†å¤±æ•—ï¼š{filepath}ï¼ŒåŸå› ï¼š{e}")

                if checked_files % 100 == 0:
                    print(f"å·²æª¢æŸ¥ {checked_files} å€‹æª”æ¡ˆ...")

    print(f"\nç¸½å…±æª¢æŸ¥ {checked_files} å€‹æª”æ¡ˆï¼Œåˆªé™¤ {len(deleted_files)} å€‹ç©ºç™½æˆ–ç´”æ›è¡Œæª”æ¡ˆã€‚")
    return deleted_files

def getOutputCsvPath(target_folder, filePrefixIdx, csvName):        
    os.makedirs(target_folder, exist_ok=True) 
    outputPath = f'{target_folder}/{str(filePrefixIdx).zfill(2)}-{csvName}.csv'
    return outputPath

# æ‰¾å‡º è§€å¯ŸæœŸ-è²·å…¥è³£å‡ºæ—¥æœŸ å°æ‡‰çš„è³‡æ–™åˆ—
def getOperiodDataRow(stock_id: str, closeDf: pd.DataFrame, baseDt: datetime, iloc: Iloc) -> pd.Series:
    dataRow = None
    candidates = closeDf[
        (closeDf['stock_id'] == stock_id)
        & (closeDf['date'].dt.year == baseDt.year)
        & (closeDf['date'].dt.month == baseDt.month)
    ]
    if not candidates.empty:
        dataRow = candidates.sort_values("date").iloc[iloc.value]
        
    if dataRow is None:
        return dataRow
    
    ### ç¢ºä¿ æœˆåˆ/æœˆåº• çš„è³‡æ–™è¦åˆ†åˆ¥è½åœ¨ç‰¹å®šçš„æ—¥æœŸå…§
    if (iloc == Iloc.Fst) and (dataRow["date"].day > 15):
        ptMsg(f'[{stock_id}]æœˆåˆè³‡æ–™æ—¥æœŸéå¤§ => {dataRow["date"].strftime("%Y%m%d")}')
        return None
    
    if (iloc == Iloc.Last) and (dataRow["date"].day < 16):
        ptMsg(f'[{stock_id}]æœˆåº•è³‡æ–™æ—¥æœŸéå° => {dataRow["date"].strftime("%Y%m%d")}')
        return None
    
    return dataRow

# æ‰¾å‡º æŒæœ‰æœŸ-è²·å…¥è³£å‡ºæ—¥æœŸ å°æ‡‰çš„è³‡æ–™åˆ—
def getHperiodDataRow(panelType: Panel, stock_id: str, closeDf: pd.DataFrame, baseDt: datetime, iloc: Iloc) -> pd.Series:
    ### Panel A
    if panelType == Panel.A:
        candidates = closeDf[
            (closeDf["stock_id"] == stock_id)
            & (closeDf["date_dt"].dt.year == baseDt.year)
            & (closeDf["date_dt"].dt.month == baseDt.month)
        ]
        if not candidates.empty:
            return candidates.sort_values("date_dt").iloc[iloc.value]

    ## Panel B
    if panelType == Panel.B:
        candidates = closeDf[
            (closeDf["stock_id"] == stock_id)
            & (closeDf["date_dt"] >= baseDt)
            & (closeDf["date_dt"] <= baseDt + timedelta(days=14))
        ]
        
        if not candidates.empty:
            return candidates.sort_values("date_dt").iloc[Iloc.Fst.value]
        
    return None

# ç™¾åˆ†æ¯”æ’å (0~100)
def scale_to_0_100(x):
    min_val = x.min()
    max_val = x.max()
    if pd.isna(min_val) or pd.isna(max_val) or max_val == min_val:
        return pd.Series([None] * len(x), index=x.index)
    else:
        return (x - min_val) / (max_val - min_val) * 100
    
# è¨ˆç®— RT_rankï¼Œæ³¨æ„ï¼šä¸å…ˆå‰µæ¬„ä½
def compute_rt_rank(group):
    mask = group["remark"] != "exclude"
    # åªé‡å°é exclude ç®—æ’å
    ranks = pd.Series(index=group.index, dtype="float")
    ranks.loc[mask] = group.loc[mask, "return"].rank(method="min", ascending=False)
    group["RT_rank"] = ranks
    return group

# æ›´æ–° remark: winner / loser
def mark_winner_loser(group):
    valid = group[group["remark"] != "exclude"]
    if valid.empty:
        return group

    n = len(valid)
    top_n = max(1, int(n * 0.1))
    bottom_n = max(1, int(n * 0.1))

    top_threshold = valid.nsmallest(top_n, "RT_rank")["RT_rank"].max()
    bottom_threshold = valid.nlargest(bottom_n, "RT_rank")["RT_rank"].min()

    # åªæ›´æ–° valid éƒ¨åˆ†
    for idx in valid.index:
        rt_rank = group.loc[idx, "RT_rank"]
        if pd.isna(rt_rank):
            continue
        if rt_rank <= top_threshold:
            group.loc[idx, "remark"] = "winner"
        elif rt_rank >= bottom_threshold and rt_rank > top_threshold:
            group.loc[idx, "remark"] = "loser"

    return group

def parse_range_from_folder(folder_name):
    """ è§£æè³‡æ–™å¤¾åç¨±ä¸­çš„ yyyymm_yyyymm ç‚º datetime å€é–“ """
    match = re.match(r"(\d{6})_(\d{6})", folder_name)
    if not match:
        return None, None
    start_str, end_str = match.groups()
    start = datetime.strptime(start_str + '01', "%Y%m%d")
    # å°‡çµæŸæœˆä»½çš„æœ€å¾Œä¸€å¤©ä½œç‚ºçµæŸæ—¥
    end = datetime.strptime(end_str + '01', "%Y%m%d")
    if end.month == 12:
        end = end.replace(month=1, year=end.year + 1)
    else:
        end = end.replace(month=end.month + 1)
    end = end.replace(day=1) - pd.Timedelta(days=1)
    return start, end

def findout_observerRTdata(output_path: str) -> bool:
    if os.path.exists(output_path):
        return True

    base_folder = os.path.dirname(output_path)
    base_folder = Path(base_folder)
    root_folder = base_folder.parent
    current_range_str = base_folder.name

    current_start, current_end = parse_range_from_folder(current_range_str)
    if current_start is None or current_end is None:
        print("âš ï¸ è³‡æ–™å¤¾åç¨±æ ¼å¼éŒ¯èª¤ï¼Œæ‡‰ç‚º yyyymm_yyyymm")
        return False

    print(f"ğŸ” è™•ç†æ™‚é–“å€é–“ï¼š{current_start.date()} ~ {current_end.date()}")

    combined_df = []

    for subfolder in root_folder.iterdir():
        if not subfolder.is_dir():
            continue
        sub_start, sub_end = parse_range_from_folder(subfolder.name)
        if sub_start is None or sub_end is None:
            continue

        # æª¢æŸ¥æ˜¯å¦æ˜¯æ¶µè“‹ç•¶å‰ç¯„åœçš„è³‡æ–™å¤¾
        if sub_start <= current_start and sub_end >= current_end:
            csv_file = subfolder / "01-observerReturnList.csv"
            if csv_file.exists():
                print(f"âœ… æ‰¾åˆ°ç¬¦åˆç¯„åœçš„æª”æ¡ˆï¼š{csv_file}")
                df = pd.read_csv(csv_file)

                # éæ¿¾ start_date èˆ‡ end_date åœ¨å€é–“å…§çš„è³‡æ–™
                df['start_date'] = pd.to_datetime(df['start_date'])
                df['end_date'] = pd.to_datetime(df['end_date'])
                df = df[(df['start_date'] >= current_start) & (df['end_date'] <= current_end)]

                combined_df.append(df)
            else:
                print(f"âŒ æ‰¾ä¸åˆ° 01-observerReturnList.csvï¼š{subfolder}")
    
    # åˆä½µè³‡æ–™ä¸¦å¯«å‡º
    if combined_df:
        result_df = pd.concat(combined_df, ignore_index=True)
        result_df.to_csv(output_path, index=False)
        print(f"ğŸ“„ å¯«å…¥æª”æ¡ˆï¼š{output_path}")
        return os.path.exists(output_path)
    
    print("âš ï¸ æ²’æœ‰æ‰¾åˆ°ä»»ä½•ç¬¦åˆæ¢ä»¶çš„è³‡æ–™")
    return False
    
def roc_to_unix(roc_date: str) -> int:
    year = None 
    month = None 
    day = None    
    seperators = ["/", ".", "-"]
    for seperator in seperators:
        if seperator in roc_date:  
            year, month, day = map(int, roc_date.split(seperator))
    if year is None:
        return None
        
    gregorian_year = year + 1911 # æ°‘åœ‹ â†’ è¥¿å…ƒï¼ˆåŠ  1911 å¹´ï¼‰
    dt = datetime(gregorian_year, month, day)
    return int(dt.timestamp())

def get_api_info(apiName: str) -> pd.DataFrame:
    sql = f"SELECT *, src_link || api_path AS url FROM data_source"
    sql += f" WHERE name = '{apiName}'"
    target = db.query_to_df(sql)
    return target

def _is_fully_in_range(sDt: datetime, eDt: datetime, minDt: datetime, maxDt: datetime) -> bool:
    """
    åˆ¤æ–·å€é–“ sDt~eDt æ˜¯å¦å®Œå…¨åŒ…å«åœ¨ minDt~maxDt å…§
    å›å‚³å¸ƒæ—å€¼
    """
    return minDt <= sDt <= maxDt and minDt <= eDt <= maxDt

def _is_no_overlap(sDt: datetime, eDt: datetime, minDt: datetime, maxDt: datetime) -> bool:
    """
    åˆ¤æ–·å€é–“ sDt~eDt æ˜¯å¦èˆ‡ minDt~maxDt å®Œå…¨ä¸é‡ç–Š
    å›å‚³å¸ƒæ—å€¼
    """
    return eDt < minDt or sDt > maxDt

def _overlap_period(sDt: datetime, eDt: datetime, minDt: datetime, maxDt: datetime):
    """
    åˆ¤æ–·å…©å€‹æ™‚é–“å€é–“æ˜¯å¦é‡ç–Šï¼Œä¸¦å›å‚³é‡ç–Šå€é–“ã€‚

    åƒæ•¸ï¼š
        sDt, eDt : datetime
        minDt, maxDt : datetime

    å›å‚³ï¼š
        è‹¥æœ‰é‡ç–Šï¼Œå›å‚³ (overlap_start, overlap_end)
        è‹¥ç„¡é‡ç–Šï¼Œå›å‚³ None
    """
    # å…ˆç¢ºä¿æ™‚é–“é †åºæ­£ç¢º
    if sDt > eDt or minDt > maxDt:
        raise ValueError("èµ·è¨–æ™‚é–“éŒ¯èª¤ï¼šstart å¿…é ˆæ—©æ–¼ end")

    # è¨ˆç®—é‡ç–Šå€é–“
    overlap_start = max(sDt, minDt)
    overlap_end = min(eDt, maxDt)

    if overlap_start <= overlap_end:
        return overlap_start, overlap_end
    else:
        return None

def chinese_to_int(s) -> int:
    # å¦‚æœæ˜¯ intï¼Œç›´æ¥è¿”å›
    if isinstance(s, int):
        return s

    # å¦‚æœæ˜¯å­—ä¸²ï¼Œå…ˆè½‰åŠå½¢
    if isinstance(s, str):
        # å…¨å½¢è½‰åŠå½¢
        s = s.translate(str.maketrans(
            "ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™",
            "0123456789"
        ))
        # åŠå½¢é˜¿æ‹‰ä¼¯æ•¸å­—ï¼Œç›´æ¥è½‰ int
        if s.isdigit():
            return int(s)

    # åŸæœ¬çš„ä¸­æ–‡æ•¸å­—è™•ç†
    num_map = {"é›¶":0, "ä¸€":1,"äºŒ":2,"ä¸‰":3,"å››":4,"äº”":5,"å…­":6,"ä¸ƒ":7,"å…«":8,"ä¹":9}
    unit_map = {"å":10, "ç™¾":100, "åƒ":1000}
    big_unit_map = {"è¬":10000, "å„„":100000000}

    def section_to_number(section: str) -> int:
        total, num = 0, 0
        for ch in section:
            if ch in num_map:
                num = num_map[ch]
            elif ch in unit_map:
                total += (num or 1) * unit_map[ch]
                num = 0
            elif ch == "é›¶":
                continue
        return total + num

    total, section = 0, ""
    for ch in reversed(s):
        if ch in big_unit_map:
            total += section_to_number(section) * big_unit_map[ch]
            section = ""
        else:
            section = ch + section
    total += section_to_number(section)
    return total
